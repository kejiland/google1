#!/usr/bin/env python3
"""
GitHub è‡ªåŠ¨èŠ‚ç‚¹è¿‡æ»¤å™¨ - ç²¾ç®€ç‰ˆ
åŠŸèƒ½ï¼šåªç§»é™¤ http=ã€https=ã€socks5= å¼€å¤´çš„èŠ‚ç‚¹ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰èŠ‚ç‚¹
"""

import requests
import re
import os
from datetime import datetime
import logging

# é…ç½®
SOURCE_URL = "https://raw.githubusercontent.com/Graysongon/google/refs/heads/main/%E4%B8%AA%E4%BA%BA"
OUTPUT_FILE = "kejiland.txt"
LOG_FILE = "filter.log"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def fetch_nodes():
    """ä»æºåœ°å€è·å–èŠ‚ç‚¹æ•°æ®"""
    try:
        logger.info(f"ğŸ“¡ æ­£åœ¨ä»æºåœ°å€è·å–æ•°æ®...")
        response = requests.get(SOURCE_URL, timeout=30)
        response.raise_for_status()
        
        # æ£€æµ‹ç¼–ç 
        response.encoding = response.apparent_encoding or 'utf-8'
        content = response.text
        
        lines = content.splitlines()
        logger.info(f"âœ… è·å–æˆåŠŸï¼å…± {len(lines)} è¡Œæ•°æ®")
        return content
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return None

def filter_nodes(content):
    """
    è¿‡æ»¤èŠ‚ç‚¹ï¼Œåªç§»é™¤ http=ã€https=ã€socks5= å¼€å¤´çš„è¡Œ
    ä¿ç•™æ‰€æœ‰å…¶ä»–æ ¼å¼çš„èŠ‚ç‚¹
    """
    if not content:
        logger.error("å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿‡æ»¤")
        return None
    
    lines = content.splitlines()
    
    filtered_lines = []
    removed_count = 0
    preserved_count = 0
    
    # è¦ç§»é™¤çš„åè®®åˆ—è¡¨ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    remove_protocols = ['http=', 'https=', 'socks5=']
    
    logger.info("ğŸ” å¼€å§‹è¿‡æ»¤èŠ‚ç‚¹...")
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        
        # è·³è¿‡ç©ºè¡Œ
        if not line_stripped:
            filtered_lines.append(line)
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦ç§»é™¤çš„åè®®
        should_remove = False
        for protocol in remove_protocols:
            if line_stripped.lower().startswith(protocol):
                should_remove = True
                removed_count += 1
                
                # è®°å½•å‰å‡ ä¸ªè¢«è¿‡æ»¤çš„èŠ‚ç‚¹
                if removed_count <= 3:
                    logger.debug(f"ç§»é™¤: {line_stripped[:60]}...")
                break
        
        if should_remove:
            continue
        
        # ä¿ç•™æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
        filtered_lines.append(line)
        preserved_count += 1
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸ“Š è¿‡æ»¤ç»Ÿè®¡")
    logger.info("=" * 60)
    logger.info(f"ğŸ“„ åŸå§‹è¡Œæ•°: {len(lines)}")
    logger.info(f"ğŸ—‘ï¸  ç§»é™¤èŠ‚ç‚¹: {removed_count} ä¸ª")
    logger.info(f"ğŸ’¾ ä¿ç•™èŠ‚ç‚¹: {preserved_count} ä¸ª")
    logger.info(f"ğŸ“ˆ ä¿ç•™æ¯”ä¾‹: {preserved_count/len(lines)*100:.1f}%")
    
    # åˆ†æä¿ç•™çš„èŠ‚ç‚¹ç±»å‹
    analyze_preserved_nodes(filtered_lines)
    
    return '\n'.join(filtered_lines)

def analyze_preserved_nodes(lines):
    """åˆ†æä¿ç•™çš„èŠ‚ç‚¹ç±»å‹"""
    logger.info("ğŸ“‹ ä¿ç•™èŠ‚ç‚¹ç±»å‹åˆ†æ:")
    
    # å¸¸è§çš„èŠ‚ç‚¹åè®®æ¨¡å¼
    protocol_patterns = {
        'ss': r'^\s*ss[:\=]',  # ss:// æˆ– ss=
        'vmess': r'^\s*vmess[:\=]',
        'vless': r'^\s*vless[:\=]',
        'trojan': r'^\s*trojan[:\=]',
        'ssr': r'^\s*ssr[:\=]',
        'hysteria': r'^\s*hysteria[:\=]',
        'tuic': r'^\s*tuic[:\=]',
        'wireguard': r'^\s*wireguard[:\=]',
        'å…¶ä»–': None  # é»˜è®¤åˆ†ç±»
    }
    
    stats = {key: 0 for key in protocol_patterns.keys()}
    stats['å…¶ä»–'] = 0
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        classified = False
        for protocol, pattern in protocol_patterns.items():
            if pattern and re.match(pattern, line_stripped, re.IGNORECASE):
                stats[protocol] += 1
                classified = True
                break
        
        if not classified and line_stripped:
            stats['å…¶ä»–'] += 1
    
    # è¾“å‡ºç»Ÿè®¡
    for protocol, count in stats.items():
        if count > 0:
            percentage = count/sum(stats.values())*100
            logger.info(f"  {protocol}: {count} ä¸ª ({percentage:.1f}%)")
    
    # æ˜¾ç¤ºä¿ç•™çš„èŠ‚ç‚¹ç¤ºä¾‹
    logger.info("\nğŸ“ ä¿ç•™èŠ‚ç‚¹ç¤ºä¾‹:")
    example_count = 0
    for line in lines:
        line_stripped = line.strip()
        if line_stripped and example_count < 5:
            # æå–åè®®éƒ¨åˆ†
            match = re.match(r'^\s*([a-zA-Z0-9]+)[:\=]', line_stripped)
            if match:
                protocol = match.group(1)
                logger.info(f"  {protocol}: {line_stripped[:70]}...")
                example_count += 1

def verify_filtering(content):
    """éªŒè¯è¿‡æ»¤ç»“æœï¼Œç¡®ä¿æ²¡æœ‰ http/https/socks5 æ®‹ç•™"""
    lines = content.splitlines()
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éœ€è¦ç§»é™¤çš„åè®®
    remaining_problems = []
    remove_protocols = ['http=', 'https=', 'socks5=']
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        for protocol in remove_protocols:
            if line_stripped.lower().startswith(protocol):
                remaining_problems.append(line_stripped)
                break
    
    if not remaining_problems:
        logger.info("âœ… éªŒè¯é€šè¿‡ï¼šæ—  HTTP/HTTPS/SOCKS5 èŠ‚ç‚¹æ®‹ç•™")
        return True
    else:
        logger.warning(f"âš ï¸  å‘ç° {len(remaining_problems)} ä¸ªæœªè¿‡æ»¤çš„èŠ‚ç‚¹:")
        for problem in remaining_problems[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            logger.warning(f"  - {problem[:60]}...")
        return False

def save_result(content):
    """ä¿å­˜è¿‡æ»¤åçš„ç»“æœåˆ°æ–‡ä»¶"""
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰å†…å®¹ç›¸åŒï¼ˆé¿å…ä¸å¿…è¦çš„å†™å…¥ï¼‰
        existing_content = ""
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                existing_content = f.read()
        
        if content == existing_content:
            logger.info("ğŸ“Œ å†…å®¹æ— å˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶")
            return False
        
        # å†™å…¥æ–°å†…å®¹
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write(content)
        
        file_size = os.path.getsize(OUTPUT_FILE)
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_FILE}")
        logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        logger.info(f"ğŸ“„ æ–‡ä»¶è¡Œæ•°: {len(content.splitlines())}")
        
        return True
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info(f"ğŸš€ GitHub èŠ‚ç‚¹è¿‡æ»¤å™¨å¯åŠ¨")
    logger.info(f"ğŸ• æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)
    
    # 1. è·å–æ•°æ®
    raw_content = fetch_nodes()
    if not raw_content:
        logger.error("æ— æ³•è·å–æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
        return False
    
    # 2. è¿‡æ»¤èŠ‚ç‚¹ï¼ˆåªç§»é™¤ http/https/socks5ï¼‰
    filtered_content = filter_nodes(raw_content)
    if not filtered_content:
        logger.error("è¿‡æ»¤å¤±è´¥")
        return False
    
    # 3. éªŒè¯è¿‡æ»¤ç»“æœ
    verify_filtering(filtered_content)
    
    # 4. ä¿å­˜ç»“æœ
    success = save_result(filtered_content)
    
    # 5. æœ€ç»ˆç»Ÿè®¡
    logger.info("=" * 60)
    if success:
        logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼æ–°å†…å®¹å·²ä¿å­˜")
    else:
        logger.info("ğŸ“ ä»»åŠ¡å®Œæˆï¼ˆæ— å†…å®¹æ›´æ–°ï¼‰")
    logger.info("=" * 60)
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)